{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: click in /home/aron/.local/lib/python3.8/site-packages (from opendatasets) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /home/aron/.local/lib/python3.8/site-packages (from opendatasets) (4.63.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2.7.3)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2.22.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.14.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.25.8)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=91ede9a35d5c46c2d01662e2b2c14b1a4a0bd19bd665a3971eaf9e01e7d49d08\n",
      "  Stored in directory: /home/aron/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
      "\u001b[33m  WARNING: The script slugify is installed in '/home/aron/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script kaggle is installed in '/home/aron/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed kaggle-1.5.12 opendatasets-0.1.22 python-slugify-6.1.2 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading speaker-recognition-dataset.zip to ./speaker-recognition-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231M/231M [00:05<00:00, 40.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "od.download(\"https://www.kaggle.com/datasets/kongaevans/speaker-recognition-dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia_Gillard\n",
      "Magaret_Tarcher\n",
      "Nelson_Mandela\n",
      "Benjamin_Netanyau\n",
      "Jens_Stoltenberg\n",
      "_background_noise_\n",
      "tf_Wav_reader.py\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'speaker-recognition-dataset/16000_pcm_speeches/tf_Wav_reader.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m list_of_np_arrays \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Get the list of files in the folder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dataset_path, folder))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Loop through the files in the folder and create a list of paths\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/BCM_activity_classification/preprocessing_speaker_rec.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m filenames \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'speaker-recognition-dataset/16000_pcm_speeches/tf_Wav_reader.py'"
     ]
    }
   ],
   "source": [
    "dataset_path = \"speaker-recognition-dataset/16000_pcm_speeches\"\n",
    "# Loop through the folders in the dataset\n",
    "for folder in os.listdir(dataset_path):\n",
    "    # Print name of the folder\n",
    "    print(folder) \n",
    "    list_of_np_arrays = []\n",
    "    # Get the list of files in the folder\n",
    "    files = os.listdir(os.path.join(dataset_path, folder))\n",
    "    # Loop through the files in the folder and create a list of paths\n",
    "    filenames = []\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == \".wav\":\n",
    "            filenames.append(os.path.splitext(file)[0])\n",
    "    \n",
    "    # Human sort the filenames\n",
    "    filenames.sort(key=lambda x: [int(c) if c.isdigit() else c for c in re.split('([0-9]+)', x)])\n",
    "        \n",
    "    for filename in filenames:\n",
    "        # Load the audio file\n",
    "        audio, sample_rate = librosa.load(os.path.join(dataset_path, folder, filename + \".wav\"), res_type=\"kaiser_fast\")\n",
    "        # Extract the MFCCs\n",
    "        list_of_np_arrays.append(audio)\n",
    "        \n",
    "    # Stack the arrays in the list into a single array\n",
    "    stacked_array = np.hstack(list_of_np_arrays)\n",
    "\n",
    "    data_mfcc = mfcc(stacked_array, samplerate = 22050, nfft = 1600, winlen=0.032, winstep=0.032, numcep=16) # Sample rate is important when using mel scale\n",
    "\n",
    "    np.save(f'data/speaker_rec/{folder}.npy', data_mfcc)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
