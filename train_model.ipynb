{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from model import LSTM_Model\n",
    "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
    "from torch import save\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'progress_bar': {'val_loss': 1.6294622421264648}, 'log': {'val_loss': 1.6294622421264648}, 'val_loss': 1.6294622421264648}\n",
      "OUR LR: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:00<00:00, 182.78it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:00<00:00, 178.94it/s]\n",
      "Learning rate set to 0.036307805477010104\n",
      "Restoring states from the checkpoint path at c:\\Users\\Aksel\\Desktop\\BCM_activity_classification\\.lr_find_d351ecc3-8fcb-460c-a0a9-7dc2549b0a7f.ckpt\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | lstm   | LSTM             | 42.0 K\n",
      "1 | fc     | Linear           | 645   \n",
      "2 | output | Sigmoid          | 0     \n",
      "3 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "42.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.6 K    Total params\n",
      "0.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 333.41it/s]{'progress_bar': {'val_loss': 1.6294622421264648}, 'log': {'val_loss': 1.6294622421264648}, 'val_loss': 1.6294622421264648}\n",
      "OUR LR: 0.036307805477010104\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1216/1216 [00:09<00:00, 126.59it/s, loss=0.905, v_num=0]{'progress_bar': {'val_loss': 1.6546919345855713}, 'log': {'val_loss': 1.6546919345855713}, 'val_loss': 1.6546919345855713}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 1: 100%|██████████| 1216/1216 [00:10<00:00, 121.40it/s, loss=0.905, v_num=0]{'progress_bar': {'val_loss': 1.6546880006790161}, 'log': {'val_loss': 1.6546880006790161}, 'val_loss': 1.6546880006790161}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 2: 100%|██████████| 1216/1216 [00:09<00:00, 132.79it/s, loss=0.905, v_num=0]{'progress_bar': {'val_loss': 1.6546863317489624}, 'log': {'val_loss': 1.6546863317489624}, 'val_loss': 1.6546863317489624}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 3: 100%|██████████| 1216/1216 [00:09<00:00, 134.75it/s, loss=1.13, v_num=0] {'progress_bar': {'val_loss': 1.6255674362182617}, 'log': {'val_loss': 1.6255674362182617}, 'val_loss': 1.6255674362182617}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 4: 100%|██████████| 1216/1216 [00:09<00:00, 124.71it/s, loss=1.13, v_num=0]{'progress_bar': {'val_loss': 1.6255675554275513}, 'log': {'val_loss': 1.6255675554275513}, 'val_loss': 1.6255675554275513}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 5: 100%|██████████| 1216/1216 [00:09<00:00, 130.43it/s, loss=1.13, v_num=0]{'progress_bar': {'val_loss': 1.6255675554275513}, 'log': {'val_loss': 1.6255675554275513}, 'val_loss': 1.6255675554275513}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 6: 100%|██████████| 1216/1216 [00:09<00:00, 133.98it/s, loss=1.13, v_num=0]{'progress_bar': {'val_loss': 1.6255680322647095}, 'log': {'val_loss': 1.6255680322647095}, 'val_loss': 1.6255680322647095}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 7: 100%|██████████| 1216/1216 [00:09<00:00, 133.68it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.6479542255401611}, 'log': {'val_loss': 1.6479542255401611}, 'val_loss': 1.6479542255401611}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 8: 100%|██████████| 1216/1216 [00:09<00:00, 127.56it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 9: 100%|██████████| 1216/1216 [00:09<00:00, 133.65it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 10: 100%|██████████| 1216/1216 [00:09<00:00, 131.77it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 11: 100%|██████████| 1216/1216 [00:09<00:00, 135.11it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 12: 100%|██████████| 1216/1216 [00:08<00:00, 135.95it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 13: 100%|██████████| 1216/1216 [00:08<00:00, 138.41it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 14: 100%|██████████| 1216/1216 [00:09<00:00, 128.36it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 15: 100%|██████████| 1216/1216 [00:08<00:00, 135.83it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 16: 100%|██████████| 1216/1216 [00:09<00:00, 133.00it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 17: 100%|██████████| 1216/1216 [00:08<00:00, 135.64it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 18: 100%|██████████| 1216/1216 [00:09<00:00, 132.08it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 19: 100%|██████████| 1216/1216 [00:09<00:00, 132.00it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 20: 100%|██████████| 1216/1216 [00:09<00:00, 131.47it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 21: 100%|██████████| 1216/1216 [00:09<00:00, 124.99it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 22: 100%|██████████| 1216/1216 [00:09<00:00, 131.84it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 23: 100%|██████████| 1216/1216 [00:09<00:00, 131.43it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 24: 100%|██████████| 1216/1216 [00:09<00:00, 134.74it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 25: 100%|██████████| 1216/1216 [00:08<00:00, 138.65it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 26: 100%|██████████| 1216/1216 [00:08<00:00, 136.43it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 27: 100%|██████████| 1216/1216 [00:08<00:00, 137.09it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 28: 100%|██████████| 1216/1216 [00:09<00:00, 132.71it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 29: 100%|██████████| 1216/1216 [00:09<00:00, 131.71it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 30: 100%|██████████| 1216/1216 [00:09<00:00, 123.15it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 31: 100%|██████████| 1216/1216 [00:10<00:00, 120.11it/s, loss=1.9, v_num=0] {'progress_bar': {'val_loss': 1.647955298423767}, 'log': {'val_loss': 1.647955298423767}, 'val_loss': 1.647955298423767}\n",
      "OUR LR: 0.036307805477010104\n",
      "Epoch 32:  40%|████      | 489/1216 [00:05<00:07, 92.11it/s, loss=1.9, v_num=0]   "
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "device = 'cpu'\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=0.00, patience=5, verbose=True, mode='min')\n",
    "model = LSTM_Model().to(device)\n",
    "#trainer = Trainer(max_epochs=100, min_epochs=1, auto_lr_find=False, auto_scale_batch_size=False, callbacks=[early_stop_callback],enable_checkpointing=False)\n",
    "trainer = Trainer(max_epochs=100, min_epochs=1, auto_lr_find=True, auto_scale_batch_size=False,enable_checkpointing=False)\n",
    "trainer.tune(model)\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "save(model.state_dict(), '/trained_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b608601bcce3d66bb15d8ba50334176c9c54db5e2e6c3520f282aef80df0bf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
